{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning using HyperDrive\n",
        "\n",
        "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.train.hyperdrive import BayesianParameterSampling\n",
        "from azureml.train.hyperdrive import uniform, choice"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1746990428909
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import csv\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import pkg_resources\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.core.dataset import Dataset"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1746990462618
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.train.hyperdrive.policy import BanditPolicy\n",
        "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
        "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
        "from azureml.train.hyperdrive.parameter_expressions import choice\n",
        "from azureml.core import ScriptRunConfig"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1746990469125
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.train.hyperdrive import (\n",
        "    PrimaryMetricGoal,\n",
        "    choice,\n",
        "    uniform,\n",
        "    loguniform\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1746990481696
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "experiment_name = 'CapstoneHyperDrive'\n",
        "\n",
        "experiment=Experiment(ws, experiment_name)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1746990488248
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: update the key to match the dataset name\n",
        "found = False\n",
        "key = \"loandataset\"\n",
        "description_text = \"Loan Dataset for Capstone Project\"\n",
        "\n",
        "if key in ws.datasets.keys(): \n",
        "        found = True\n",
        "        dataset = ws.datasets[key] \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = dataset.to_pandas_dataframe()\n",
        "df.describe()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'infer_column_types': 'False', 'activity': 'to_pandas_dataframe'}\n{'infer_column_types': 'False', 'activity': 'to_pandas_dataframe', 'activityApp': 'TabularDataset'}\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "         person_age  person_income  person_emp_exp     loan_amnt  \\\ncount  45000.000000   4.500000e+04    45000.000000  45000.000000   \nmean      27.764178   8.031905e+04        5.410333   9583.157556   \nstd        6.045108   8.042250e+04        6.063532   6314.886691   \nmin       20.000000   8.000000e+03        0.000000    500.000000   \n25%       24.000000   4.720400e+04        1.000000   5000.000000   \n50%       26.000000   6.704800e+04        4.000000   8000.000000   \n75%       30.000000   9.578925e+04        8.000000  12237.250000   \nmax      144.000000   7.200766e+06      125.000000  35000.000000   \n\n       loan_int_rate  loan_percent_income  cb_person_cred_hist_length  \\\ncount   45000.000000         45000.000000                45000.000000   \nmean       11.006606             0.139725                    5.867489   \nstd         2.978808             0.087212                    3.879702   \nmin         5.420000             0.000000                    2.000000   \n25%         8.590000             0.070000                    3.000000   \n50%        11.010000             0.120000                    4.000000   \n75%        12.990000             0.190000                    8.000000   \nmax        20.000000             0.660000                   30.000000   \n\n       credit_score   loan_status  \ncount  45000.000000  45000.000000  \nmean     632.608756      0.222222  \nstd       50.435865      0.415744  \nmin      390.000000      0.000000  \n25%      601.000000      0.000000  \n50%      640.000000      0.000000  \n75%      670.000000      0.000000  \nmax      850.000000      1.000000  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>person_age</th>\n      <th>person_income</th>\n      <th>person_emp_exp</th>\n      <th>loan_amnt</th>\n      <th>loan_int_rate</th>\n      <th>loan_percent_income</th>\n      <th>cb_person_cred_hist_length</th>\n      <th>credit_score</th>\n      <th>loan_status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>45000.000000</td>\n      <td>4.500000e+04</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>27.764178</td>\n      <td>8.031905e+04</td>\n      <td>5.410333</td>\n      <td>9583.157556</td>\n      <td>11.006606</td>\n      <td>0.139725</td>\n      <td>5.867489</td>\n      <td>632.608756</td>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>6.045108</td>\n      <td>8.042250e+04</td>\n      <td>6.063532</td>\n      <td>6314.886691</td>\n      <td>2.978808</td>\n      <td>0.087212</td>\n      <td>3.879702</td>\n      <td>50.435865</td>\n      <td>0.415744</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>20.000000</td>\n      <td>8.000000e+03</td>\n      <td>0.000000</td>\n      <td>500.000000</td>\n      <td>5.420000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>390.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>24.000000</td>\n      <td>4.720400e+04</td>\n      <td>1.000000</td>\n      <td>5000.000000</td>\n      <td>8.590000</td>\n      <td>0.070000</td>\n      <td>3.000000</td>\n      <td>601.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>26.000000</td>\n      <td>6.704800e+04</td>\n      <td>4.000000</td>\n      <td>8000.000000</td>\n      <td>11.010000</td>\n      <td>0.120000</td>\n      <td>4.000000</td>\n      <td>640.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>30.000000</td>\n      <td>9.578925e+04</td>\n      <td>8.000000</td>\n      <td>12237.250000</td>\n      <td>12.990000</td>\n      <td>0.190000</td>\n      <td>8.000000</td>\n      <td>670.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>144.000000</td>\n      <td>7.200766e+06</td>\n      <td>125.000000</td>\n      <td>35000.000000</td>\n      <td>20.000000</td>\n      <td>0.660000</td>\n      <td>30.000000</td>\n      <td>850.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1746990502498
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create director to store job files\n",
        "\n",
        "import os\n",
        "\n",
        "train_src_dir = \"./scripts\"\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1746990520770
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: xgboost in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (1.5.2)\r\nRequirement already satisfied: scipy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (1.11.0)\r\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (1.23.5)\r\n"
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from azureml.core.run import Run\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from azureml.core import Run, Dataset, Workspace\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Save model for current iteration\n",
        "run = Run.get_context()\n",
        "\n",
        "def main():\n",
        "    # Add arguments to script\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str)\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.1, help=\"Step size shrinkage to prevent overfitting\")\n",
        "    parser.add_argument('--n_estimators', type=int, default=100, help=\"Number of boosting rounds\")\n",
        "    parser.add_argument('--max_depth', type=int, default=3, help=\"Maximum depth of a tree\")\n",
        "    parser.add_argument('--random_state', type=int, default=1, help=\"Random seed for reproducibility\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    run.log(\"Learning Rate:\", np.float(args.learning_rate))\n",
        "    run.log(\"Number of Estimators:\", np.int(args.n_estimators))\n",
        "    run.log(\"Max Depth:\", np.int(args.max_depth))\n",
        "    run.log(\"Random State:\", np.int(args.random_state))\n",
        "\n",
        "    workspace = run.experiment.workspace  # Get the workspace\n",
        "    dataset_name = args.data # Get dataset name from arguments\n",
        "\n",
        "    # Get the Tabular Dataset and convert to DataFrame\n",
        "    tabular_dataset = Dataset.get_by_name(workspace, name=dataset_name)\n",
        "    x_df = tabular_dataset.to_pandas_dataframe()\n",
        "\n",
        "    y_df = x_df.pop(\"loan_status\")\n",
        "\n",
        "    # Identify numeric and categorical columns\n",
        "    numeric_cols = x_df.select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_cols = x_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Scale numeric columns\n",
        "    if numeric_cols:  # Check if there are any numeric columns\n",
        "        scaler = StandardScaler()\n",
        "        x_df[numeric_cols] = scaler.fit_transform(x_df[numeric_cols])\n",
        "  \n",
        "    # One-hot encode categorical columns\n",
        "    if categorical_cols: #check if there are categorical columns\n",
        "        x_df = pd.get_dummies(x_df, columns=categorical_cols, drop_first=True)\n",
        "  \n",
        "\n",
        "\n",
        "    # Split data into train and test sets.\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.3, random_state=args.random_state)\n",
        "\n",
        "    model = xgb.XGBClassifier(\n",
        "        learning_rate=args.learning_rate,\n",
        "        n_estimators=args.n_estimators,\n",
        "        max_depth=args.max_depth,\n",
        "        random_state=args.random_state\n",
        "    ).fit(x_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    run.log(\"Accuracy\", np.float(accuracy))\n",
        "\n",
        "    # Save model for current iteration, also include the hyperparameters in filename\n",
        "    os.makedirs('outputs', exist_ok=True)\n",
        "    joblib.dump(model, f'outputs/xgboost_lr_{args.learning_rate}_nestimators_{args.n_estimators}_depth_{args.max_depth}_rs_{args.random_state}.joblib')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./scripts/train.py\n"
        }
      ],
      "execution_count": 65,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperdrive Configuration\n",
        "\n",
        "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings."
      ],
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1598531923519
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.train.hyperdrive import BayesianParameterSampling\n",
        "from azureml.train.hyperdrive import uniform, choice\n"
      ],
      "outputs": [],
      "execution_count": 52,
      "metadata": {
        "gather": {
          "logged": 1746998522734
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_sampling = RandomParameterSampling(\n",
        "        {\n",
        "            '--learning_rate': loguniform(-3, -1),  # Example of log uniform distribution\n",
        "            '--n_estimators': choice(50, 100, 150, 200),\n",
        "            '--max_depth': choice(3, 4, 5, 6),\n",
        "            \n",
        "        }\n",
        "    )\n",
        "\n",
        "# Specify the primary metric to optimize.\n",
        "primary_metric_name = 'Accuracy'  # The name should match what your training script logs.\n",
        "primary_metric_goal = PrimaryMetricGoal.MAXIMIZE"
      ],
      "outputs": [],
      "execution_count": 66,
      "metadata": {
        "gather": {
          "logged": 1746999812688
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_termination_policy = BanditPolicy(evaluation_interval=1, slack_factor=0.2, delay_evaluation=5)\n"
      ],
      "outputs": [],
      "execution_count": 67,
      "metadata": {
        "gather": {
          "logged": 1746999815162
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "# Create an Azure ML environment\n",
        "myenv = Environment(name='my-xgboost-env')\n",
        "# Create a conda environment specification\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_conda_package('scikit-learn')\n",
        "conda_dep.add_conda_package('pandas')\n",
        "conda_dep.add_conda_package('xgboost')\n",
        "myenv.python.conda_dependencies = conda_dep"
      ],
      "outputs": [],
      "execution_count": 69,
      "metadata": {
        "gather": {
          "logged": 1746999819091
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training environment\n",
        "script_config = ScriptRunConfig(\n",
        "        source_directory=train_src_dir,  # The folder containing your training script\n",
        "        script='train.py',  # The name of your training script\n",
        "        compute_target=\"project2\",\n",
        "        environment=myenv,\n",
        "        arguments=[\n",
        "            '--data',\n",
        "            'loandataset',  #  dataset name\n",
        "        ],\n",
        "    )\n",
        "    # Configure HyperDrive\n",
        "hyperdrive_config = HyperDriveConfig(\n",
        "        policy=early_termination_policy,\n",
        "        hyperparameter_sampling=param_sampling,\n",
        "        primary_metric_name=primary_metric_name,\n",
        "        primary_metric_goal=primary_metric_goal,\n",
        "        run_config=script_config,\n",
        "        max_total_runs=20,  # Maximum number of hyperparameter combinations to try\n",
        "        max_concurrent_runs=2,  # Adjust based on your compute target's capabilities\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": 70,
      "metadata": {
        "gather": {
          "logged": 1746999822070
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Submit your experiment\n",
        "hyperDrive_run = experiment.submit(hyperdrive_config)\n"
      ],
      "outputs": [],
      "execution_count": 71,
      "metadata": {
        "gather": {
          "logged": 1746999825958
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Details\n",
        "\n",
        "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
        "\n",
        "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
      ],
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1598544898497
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(hyperDrive_run).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746995647545
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show scikit-learn\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Name: scikit-learn\r\nVersion: 1.5.1\r\nSummary: A set of python modules for machine learning and data mining\r\nHome-page: https://scikit-learn.org\r\nAuthor: \r\nAuthor-email: \r\nLicense: new BSD\r\nLocation: /anaconda/envs/azureml_py38/lib/python3.10/site-packages\r\nRequires: joblib, numpy, scipy, threadpoolctl\r\nRequired-by: azureml-automl-runtime, azureml-train-automl-runtime, azureml-training-tabular, dice-ml, econml, erroranalysis, fairlearn, interpret-core, interpret_community, ml_wrappers, pmdarima, raiutils, raiwidgets, responsibleai, seqeval, shap, skl2onnx, sklearn-pandas\r\n"
        }
      ],
      "execution_count": 38,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Model\n",
        "\n",
        "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "best_run = hyperDrive_run.get_best_run_by_primary_metric()\n",
        "best_run_metrics = best_run.get_metrics()\n",
        "\n",
        "print('Best Run Id: ', best_run.id)\n",
        "print('Best Run Metrics:', best_run_metrics)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Best Run Id:  HD_58087a3f-96de-4ff8-a6fb-22f234f55311_9\nBest Run Metrics: {'Learning Rate:': 0.20616888331427718, 'Max Depth:': 6, 'Number of Estimators:': 150, 'Random State:': 1, 'Accuracy': 0.9317037037037037}\n"
        }
      ],
      "execution_count": 80,
      "metadata": {
        "gather": {
          "logged": 1747001078547
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_run_metrics"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 78,
          "data": {
            "text/plain": "{'Learning Rate:': 0.20616888331427718,\n 'Max Depth:': 6,\n 'Number of Estimators:': 150,\n 'Random State:': 1,\n 'Accuracy': 0.9317037037037037}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 78,
      "metadata": {
        "gather": {
          "logged": 1747001031149
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Deployment\n",
        "\n",
        "Remember you have to deploy only one of the two models you trained but you still need to register both the models. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
        "\n",
        "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the model to deploy\n",
        "hyperdrive_model = best_run.register_model(model_name = \"hyperdrive_capstone_bestmodel\", model_path = 'outputs/')\n",
        "print(hyperdrive_model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model(workspace=Workspace.create(name='mlcloud', subscription_id='d990bb6c-7849-4109-9dd7-6cafa051c8ae', resource_group='mlcloud'), name=hyperdrive_capstone_bestmodel, id=hyperdrive_capstone_bestmodel:1, version=1, tags={}, properties={})\n"
        }
      ],
      "execution_count": 82,
      "metadata": {
        "gather": {
          "logged": 1747001305859
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submission Checklist**\n",
        "- I have registered the model.\n",
        "- I have deployed the model with the best accuracy as a webservice.\n",
        "- I have tested the webservice by sending a request to the model endpoint.\n",
        "- I have deleted the webservice and shutdown all the computes that I have used.\n",
        "- I have taken a screenshot showing the model endpoint as active.\n",
        "- The project includes a file containing the environment details.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}